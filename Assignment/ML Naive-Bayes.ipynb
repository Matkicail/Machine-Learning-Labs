{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive-Bayes Classifer for \"Fake News\" Dataset\n",
    "\n",
    "The classes we are attempting to predict are:\n",
    "- 0 => reliable news\n",
    "- 1 => unreliable/fake news\n",
    "\n",
    "## 3 steps to Naive-Bayes\n",
    "\n",
    "- **Step 1:** *Separate dataset by class*\n",
    "- **Step 2:** *Summarise dataset (i.e. calculate probabilties)*\n",
    "- **Step 3:** *Predict for new data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports go here\n",
    "\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "import numpy as np\n",
    "import math\n",
    "from time import sleep\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "##Imports the song playing capacity.\n",
    "import webbrowser\n",
    "\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    return pd.read_csv(name, names = ['ID','Title','Author','Text','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStringWords(string, badWords):\n",
    "    size = len(badWords)\n",
    "    for i in range(size):\n",
    "        try:\n",
    "            string.remove(badWords[i])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndices(dataSize, percent):\n",
    "    sizeChoice = int(dataSize * 0.2)\n",
    "    randomChoices = random.sample(range(dataSize-1),sizeChoice)\n",
    "    half = int(sizeChoice/2)\n",
    "    firstHalf = randomChoices[0:half]\n",
    "    secondHalf = randomChoices[half:]\n",
    "    return firstHalf, secondHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSets(totData, percent):\n",
    "    dataSize = len(totData)\n",
    "    testIndices, validIndices = createIndices(dataSize, percent)\n",
    "    trainIndices = np.empty(0)\n",
    "    for i in range(dataSize):\n",
    "        \n",
    "        if i not in testIndices and i not in validIndices:\n",
    "            trainIndices = np.append(trainIndices, i)\n",
    "            \n",
    "    testFrame = totData.drop(trainIndices)\n",
    "    testFrame = testFrame.drop(validIndices)\n",
    "    validFrame = totData.drop(trainIndices)\n",
    "    validFrame = validFrame.drop(testIndices)\n",
    "    trainFrame = totData.drop(testIndices)\n",
    "    trainFrame = trainFrame.drop(validIndices)\n",
    "    return trainFrame, testFrame, validFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jesse's method for cleaning data\n",
    "\n",
    "#remove NaN, punctuation and new lines\n",
    "def getCleanedData(data):\n",
    "    feature_names = np.array(['ID', 'Title', 'Author', 'Text'])\n",
    "    \n",
    "    #convert NaNs in order to remove later\n",
    "    for i in range(3):\n",
    "        string_replacement = \"\"\n",
    "        if i == 1:\n",
    "            string_replacement = \"-NO AUTHOR-\"\n",
    "        else:\n",
    "            string_replacement = \"NaN\"\n",
    "            \n",
    "        for j in range(len(data)):\n",
    "            if pd.isnull(data[j][i+1]):\n",
    "                data[j][2] = string_replacement\n",
    "                \n",
    "    #punctuation to remove\n",
    "    remove = string.punctuation\n",
    "    remove = remove + \"“\"\n",
    "    remove = remove + \"”\"\n",
    "    remove = remove + \"’\"\n",
    "    remove = remove + '‘'\n",
    "    remove = remove + '—'\n",
    "    remove = remove + '–'\n",
    "    \n",
    "    #remove rows that contain NaN\n",
    "    data = data[np.all(data != \"NaN\", axis = 1)]\n",
    "    \n",
    "    #remove punctuation, new lines, and convert words to lowercase\n",
    "    for i in range(3):\n",
    "        for j in range(len(data)):\n",
    "            data[j][i+1] = data[j][i+1].replace(\"\\n\",\"\").translate(str.maketrans('', '', remove)).lower()\n",
    "            \n",
    "    #remove rows that contain '' after removing a bunch of things\n",
    "    data = data[np.all(data != '', axis = 1)]\n",
    "    return feature_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(text):\n",
    "    badWords = ['not','you','at','from','of','us','in','have','yes','no','are','','for','but','that','it','this','he','she','they','that','a','an','who','where','there','his','her','their','i','my','we','our','were','the','if','as','and','in','on','we','to','also','so','is','its']\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.split(' ')\n",
    "        text = removeStringWords(text, badWords)\n",
    "        #print('success')\n",
    "        #sleep(0.01)\n",
    "        return text\n",
    "    except:\n",
    "        #print('fail')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArr(text, y):\n",
    "    return text.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounters(data, title, labelTitle, labelDesired):\n",
    "        dataTitle = data.loc[data[labelTitle] == labelDesired]\n",
    "        titleArray = dataTitle[title].to_numpy()\n",
    "        results = []\n",
    "        for i in titleArray:\n",
    "            y = delayed(cleanString)(i)\n",
    "            try:\n",
    "                p = float(y[0])\n",
    "                pass\n",
    "            except:\n",
    "                if y is not None:\n",
    "                    #print(y)\n",
    "                    appendArr(results,y)\n",
    "                \n",
    "        texts = delayed(results)\n",
    "        return texts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArr(text, y):\n",
    "    return text.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCounters(prevCount, currCount):\n",
    "    return prevCount + currCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempNameCounter(dataFrame, title, labelTitle, labelDesired):\n",
    "    titleCounterReal = getCounters(trainFrame, title, labelTitle, labelDesired)\n",
    "    results = []\n",
    "    resultstwo = []\n",
    "    j = 0\n",
    "    for i in titleCounterReal:\n",
    "        y = delayed(count_words)(i)\n",
    "        if j % 2:\n",
    "            results.append(y)\n",
    "        else:\n",
    "            resultstwo.append(y) \n",
    "    bigCount = delayed(addCounters)(results,resultstwo)\n",
    "    done = bigCount.compute() \n",
    "    #bigCount.visualize()\n",
    "    sadMe = Counter()\n",
    "    for i in done:\n",
    "        sadMe = sadMe + count_words(i)\n",
    "    return sadMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantInfo(dataFrame, title, labelTitle, labels):\n",
    "    results = []\n",
    "    j = 0\n",
    "    for i in labels:\n",
    "        #print(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        results.append(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        #print(\"result at index \" + str(j) + \" corresponds to \" + title + \" of label \" + str(i))\n",
    "        j +=1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitles(dataFrame):\n",
    "    print(\"Running Titles\")\n",
    "    return  getRelevantInfo(dataFrame, 'Title', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Text', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthor(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Author', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(data):\n",
    "    wordsReal, numberReal = [list(c) for c in zip(*list(data[0].items()))]\n",
    "    wordsFake, numberFake = [list(c) for c in zip(*list(data[1].items()))]\n",
    "    return [wordsReal,numberReal,wordsFake,numberFake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAllDataString(results):\n",
    "    #use the longest array to do the least work\n",
    "    allWords = []\n",
    "    allNumbers = []\n",
    "    realWords = results[0]\n",
    "    realNumbers = results[1]\n",
    "    fakeWords = results[2]\n",
    "    fakeNumbers = results[3]\n",
    "    allWords = fakeWords\n",
    "    allNumbers = fakeNumbers\n",
    "   # print(fakeWords)\n",
    "   # print(realWords)\n",
    "    for i in realWords:\n",
    "        indexReal = realWords.index(i)\n",
    "        if i in allWords:\n",
    "            indexAll = allWords.index(i)\n",
    "            allNumbers[indexAll] = allNumbers[indexAll] + realNumbers[indexReal]\n",
    "        else:\n",
    "            #print(\"added Word\")\n",
    "            allWords.append(i)\n",
    "            allNumbers.append(realNumbers[indexReal])\n",
    "            \n",
    "                \n",
    "    #to verify just check that the lengths match up, the assumption is that there will be a smaller all words compared to realWords + fakeWords\n",
    "    #print(len(allWords))\n",
    "    #print(len(realWords) + len(fakeWords))\n",
    "    return allWords, allNumbers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please only use this to test to see if it generates the binary data correctly it is referenced in createBinaryDataStrings.\n",
    "#The idea is that should we have all the words captured the result array since it is constituted of words should be equal to the length of the the two sets of words at it's max but obviously\n",
    "#we expect it to be a bit or a lot smaller than it, so I made a way of testing it to see that everytime it finds a word it counts it as a 1(for being true in the array)\n",
    "#if the printed counter is not the same as teh array in size then clearly something must be missing.\n",
    "#hence why this will definitely show that the above works\n",
    "def testCreateBinaryDataStrings(realArray, fakeArray, resultArray):\n",
    "    print(len(realArray))\n",
    "    print(len(fakeArray))\n",
    "    j = 0 \n",
    "    for i in realArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    j = 0\n",
    "    for i in fakeArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    print(len(resultArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotCodeWordArray(dataPoint, allData):\n",
    "    # allData = np.append(allData, \"SEXYGODZILLA\") this is just me checking if it works\n",
    "    binary = np.ones(0, dtype = bool)\n",
    "    for i in allData:\n",
    "        if i in dataPoint:\n",
    "            binary = np.append(binary, True)\n",
    "        else:\n",
    "            binary = np.append(binary,False)\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playSongWhenDone():\n",
    "    # feel free to add songs or remove them as you want, just follow the format of nextNum,songLink\n",
    "    temp = pd.read_csv('songs.csv', names = ['id', 'url'])\n",
    "    ID = temp['id'].to_numpy()\n",
    "    urls = temp['url'].to_numpy()\n",
    "    i = random.sample(range(len(ID)),1)\n",
    "    song = urls[i]\n",
    "    try:\n",
    "        webbrowser.open(song[0])\n",
    "    except:\n",
    "        print(\"no internet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Separate dataset by class\n",
    "\n",
    "Done by using the unzip() and getRelevantInfo() methods\n",
    "\n",
    "We also separate our data into training, testing, and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load dataset from CSV file\n",
    "filename = 'tenPercent.csv'\n",
    "data = loadData(filename)\n",
    "\n",
    "# create training, testing, and validation datasets from dataset 'data'\n",
    "trainFrame, testFrame, validFrame = createDataSets(data, 0.2)\n",
    "\n",
    "# this returns an array of arrays of the form [realWords, realNumbers, fakeWords, fakeNumbers]\n",
    "# realWords => words occurring in data classed as real, similarly for fakeWords\n",
    "# realNumbers => number of occurrences of words in realWords, similarly for fakeNumbers\n",
    "# i.e. realNumbers[0] = \"apple\", realNumbers[0] = 5 means that the word \"apple\" occurs 5 times in articles classed as real news.\n",
    "# trainTextData = unzip(getText(trainFrame))\n",
    "\n",
    "\n",
    "# separate training data by class\n",
    "# loc gets every row where the column value meets some condition\n",
    "\n",
    "#I understand you are doing this for your class priors so cool\n",
    "real = trainFrame.loc[trainFrame['Label'] == 0]\n",
    "fake = trainFrame.loc[trainFrame['Label'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Text\n",
      "Running Titles\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainTextData = unzip(getText(trainFrame))\n",
    "trainTitleData = unzip(getTitles(trainFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 334 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allTitles = createAllDataString(trainTitleData)\n",
    "# createAllDataString creates a 2D array of [all unique words in dataset, occurrences of unique words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allText = createAllDataString(trainTextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#hotTitlesReal = hotCodeWordArray(trainTitleData[0], allTitles[0])\n",
    "#hotTitlesFake = hotCodeWordArray(trainTitleData[2], allTitles[0])\n",
    "#hotTextReal = hotCodeWordArray(trainTitleData[0], allText[0])\n",
    "#hotTextFake = hotCodeWordArray(trainTitleData[2], allText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly separate and store real words, fake words and their occurrences\n",
    "\n",
    "realTextWords = trainTextData[0]\n",
    "realTextNumbers = trainTextData[1]\n",
    "fakeTextWords = trainTextData[2]\n",
    "fakeTextNumbers = trainTextData[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Summarise the dataset (calculate probabilities)\n",
    "\n",
    "We need:\n",
    "   - the probability that an article is real or fake news\n",
    "   - for each article, a vector of words that occur in it (vocabulary)\n",
    "   - a vocab vector for real and fake, and the number of occurences of each word. (This is done already above.) The global vocabulary is just the union of realWords and fakeWords.\n",
    "   \n",
    "P(class given data) = ( product of P(Xi given class) * P(class) ) / P(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49278846 0.50721154]\n"
     ]
    }
   ],
   "source": [
    "# get total number of articles in training set\n",
    "dataSize = len(trainFrame) \n",
    "\n",
    "# get number of real articles and number of fake articles\n",
    "#this is fine because of what you did initially\n",
    "numReal = len(real)\n",
    "numFake = len(fake)\n",
    "\n",
    "classPriors = np.array([(numReal/dataSize),(numFake/dataSize)])\n",
    "print(classPriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get class conditional table\n",
    "\n",
    "To do this we need:\n",
    "- priors for each class\n",
    "- to be able to vectorise the text of an article to show what words it contains that we have seen before.\n",
    "    - this then means that we need to vectorise the text from our training articles as well, or some way to check if a training article contains a word we have seen in the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class conditional table\n",
    "# need for each word, (instances in class/classSize) for each class\n",
    "\n",
    "\n",
    "#This below feels super dodgy given how the unzips work and especially given how the getText function works\n",
    "#realText = unzip(getText(real))\n",
    "#fakeText = unzip(getText(fake))\n",
    "#Yeah it is pretty dodgy, this is what you want, you basically told it to sepukku but dishonourably. \n",
    "#I ran it and got an infinite loop.\n",
    "\n",
    "#realTextWords = trainTextData[0]\n",
    "#realTextNumbers = trainTextData[1]\n",
    "#fakeTextWords = trainTextData[2]\n",
    "#fakeTextNumbers = trainTextData[3]\n",
    "\n",
    "\n",
    "# realText[0] => every unique word[i] in real articles\n",
    "# realText[1] => number of occurrences of word[i] in real articles\n",
    "# similarly for fakeText[0] and fakeText[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert each article into boolean/binary vector words ---- I tried this, had a couple issues but I made it into a binary vector, \n",
    "# words encodes whether an article contains a word or not, for all words we have seen\n",
    "\n",
    "\n",
    "#Here is what I did\n",
    "#you dont want to use this for that\n",
    "#I recommend the following\n",
    "\n",
    "#allTitlesInfo = unzip(getTitles(trainFrame))\n",
    "#allTextInfo = unzip(getText(trainFrame)) #Text will always take a bit of time\n",
    "#allAuthorInfo = unzip(getAuthor(trainFrame))\n",
    "\n",
    "#the results for the above have the following form, index 0 = true words\n",
    "#index 0 = true words, index 1 = true words' respective counts \n",
    "#index 2 = fake words, index 3 = fake words' respective counts\n",
    "\n",
    "#please avoid calling these get functions, use them once and then store them, they are quite computationally expensive and when you move on to\n",
    "#the Actual Data, you might take ~30 min to do it once on getText. So always store.\n",
    "# createBinaryAllDataString(getText(trainFrame))\n",
    "\n",
    "#I will segment it here for that reason so that all our gets are in one area so that we dont have to wait ages to continue.\n",
    "#Try and keep things that are expensive in their smallish segments so you dont have to worry about them too much\n",
    "\n",
    "realTitleWords = trainTitleData[0]\n",
    "realTitleNumbers = trainTitleData[1]\n",
    "fakeTitleWords = trainTitleData[2]\n",
    "fakeTitleNumbers = trainTitleData[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What this does is not a vectorise anymore, and since I need to try and save on memory space at times to help you down the road I changed the vectorised function.\n",
    "#What this does is it creates 2 arrays, one array containing ALL words for both the fake and real entries of that piece you gave it.\n",
    "#The other array is the array containing those counts, in order.\n",
    "#it will seperate it for you, so long as you give it an all_____info where that ____ is whatever you wanted text,title or author\n",
    "allTitles = createAllDataString(trainTitleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next you have the vectorise, so the vectorise function does things a bit weirder but regardless will work on any string vector you give it.\n",
    "\n",
    "\n",
    "hotTitlesTrue = hotCodeWordArray(trainTitleData[0], allTitles[0])\n",
    "hotTitlesFake = hotCodeWordArray(trainTitleData[2], allTitles[0])\n",
    "hotTextTrue = hotCodeWordArray(trainTitleData[0], allText[0])\n",
    "hotTextFalse = hotCodeWordArray(trainTitleData[2], allText[0])\n",
    "\n",
    "#the inputs are as follows, the result_____ (0 is for only real strings of that result and 2 for only fake strings of that result)\n",
    "#always all____[0] since this contains all the strings for that thing. Whereas [1] contains all the counts\n",
    "#This is all just a boolean set.\n",
    "\n",
    "\n",
    "#if you give it a new entry it will work just fine, I recommend looking at your interaction with pandas that you be a little careful on how you interact with it.\n",
    "#I personally would get it to to read a csv file similar to load data and then break it up into arrays where each array is that respective thing\n",
    "#I.e a author, title, text and label array (which you can hide for testing but use in validation)\n",
    "# a functional showcase is in this function - https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
    "#you should want to do that and just give it the right data.\n",
    "\n",
    "\n",
    "# I need to get each article as a boolean vector\n",
    "# related to all the words we have seen over all of the articles, real and fake\n",
    "\n",
    "# then iterate over each article's vector\n",
    "# count articles that contain each word for real and for fake\n",
    "# these count values go into the class conditional table\n",
    "# classCon[0][0] => (# of real articles containing word 0)/(# of real articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas row:\n",
      "ID                                                       28\n",
      "Title     Andrea Tantaros of Fox News Claims Retaliation...\n",
      "Author                                            Jim Dwyer\n",
      "Text      Andrea Tantaros, a former Fox News host, charg...\n",
      "Label                                                     0\n",
      "Name: 28, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# just get first real news article for testing vectorisation\n",
    "#r = real.iloc[0]\n",
    "#print(\"Pandas row:\")\n",
    "#print(row)\n",
    "\n",
    "# attempt to unzip text from r, since r is still in Pandas format\n",
    "#try not to, do a to_numpy on the columns you are interested and get an array for each, they are all the same length so it is okay and you could index all using one variable with their respective arrays\n",
    "#This means you can have 3 naibe bayes that tell you the probability of it being a Real on title, on text and on author seperately\n",
    "#Should help you find which is the best for seeing which is true, since if you want to do it on three seperate variables, you will need to look into statistics for a dependency to normalise this (which I know is not wanted)\n",
    "#It is possible but it will be a thing we can do right at the end since they are independent up till then since you cant multiply those 3 probabilities by eachother without thinking about the dependency\n",
    "\n",
    "#so it would be like P(real|title), p(real|text), p(real|author) since if you multiplied those you would unless all Ps are 1 would reduce probability and \n",
    "# p(real|title)*p(real|text)*p(real|author) + p(fake|title)*p(fake|text)*p(fake|author) != 1 (almost always) and hence a need to normalise and hence why I say dont try do that\n",
    "#make one naive bayes function that is robus and use it on each of them.\n",
    "\n",
    "# ^^^ this last line is the plan, I'm testing on the text since that seems to be running quickly enough\n",
    "# though I see you are only testing on titles really for quicker testing\n",
    "# it's a bit hard to keep track of the million different variables created, a lot of them redundant\n",
    "\n",
    "# article in numpy array form\n",
    "#realArr = real.to_numpy()\n",
    "#print()\n",
    "#print(\"Numpy row:\")\n",
    "#print(realArr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from row 0\n",
      "Training from row 1\n",
      "Training from row 2\n",
      "Training from row 3\n",
      "Training from row 4\n",
      "Training from row 5\n",
      "Training from row 6\n",
      "Training from row 7\n",
      "Training from row 8\n",
      "Training from row 9\n",
      "Training from row 10\n",
      "Training from row 11\n",
      "Training from row 12\n",
      "Training from row 13\n",
      "Training from row 14\n",
      "Training from row 15\n",
      "Training from row 16\n",
      "Training from row 17\n",
      "Training from row 18\n",
      "Training from row 19\n",
      "Training from row 20\n",
      "Training from row 21\n",
      "Training from row 22\n",
      "Training from row 23\n",
      "Training from row 24\n",
      "Training from row 25\n",
      "Training from row 26\n",
      "Training from row 27\n",
      "Training from row 28\n",
      "Training from row 29\n",
      "Training from row 30\n",
      "Training from row 31\n",
      "Training from row 32\n",
      "Training from row 33\n",
      "Training from row 34\n",
      "Training from row 35\n",
      "Training from row 36\n",
      "Training from row 37\n",
      "Training from row 38\n",
      "Training from row 39\n",
      "Training from row 40\n",
      "Training from row 41\n",
      "Training from row 42\n",
      "Training from row 43\n",
      "Training from row 44\n",
      "Training from row 45\n",
      "Training from row 46\n",
      "Training from row 47\n",
      "Training from row 48\n",
      "Training from row 49\n",
      "Training from row 50\n",
      "Training from row 51\n",
      "Training from row 52\n",
      "Training from row 53\n",
      "Training from row 54\n",
      "Training from row 55\n",
      "Training from row 56\n",
      "Training from row 57\n",
      "Training from row 58\n",
      "Training from row 59\n",
      "Training from row 60\n",
      "Training from row 61\n",
      "Training from row 62\n",
      "Training from row 63\n",
      "Training from row 64\n",
      "Training from row 65\n",
      "Training from row 66\n",
      "Training from row 67\n",
      "Training from row 68\n",
      "Training from row 69\n",
      "Training from row 70\n",
      "Training from row 71\n",
      "Training from row 72\n",
      "Training from row 73\n",
      "Training from row 74\n",
      "Training from row 75\n",
      "Training from row 76\n",
      "Training from row 77\n",
      "Training from row 78\n",
      "Training from row 79\n",
      "Training from row 80\n",
      "Training from row 81\n",
      "Training from row 82\n",
      "Training from row 83\n",
      "Training from row 84\n",
      "Training from row 85\n",
      "Training from row 86\n",
      "Training from row 87\n",
      "Training from row 88\n",
      "Training from row 89\n",
      "Training from row 90\n",
      "Training from row 91\n",
      "Training from row 92\n",
      "Training from row 93\n",
      "Training from row 94\n",
      "Training from row 95\n",
      "Training from row 96\n",
      "Training from row 97\n",
      "Training from row 98\n",
      "Training from row 99\n",
      "Training from row 100\n",
      "Training from row 101\n",
      "Training from row 102\n",
      "Training from row 103\n",
      "Training from row 104\n",
      "Training from row 105\n",
      "Training from row 106\n",
      "Training from row 107\n",
      "Training from row 108\n",
      "Training from row 109\n",
      "Training from row 110\n",
      "Training from row 111\n",
      "Training from row 112\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-ed8051f163fe>\u001b[0m in \u001b[0;36mhotCodeWordArray\u001b[1;34m(dataPoint, allData)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataPoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# need to count occurrences of each word in real and fake articles\n",
    "# this is where the actual training happens\n",
    "\n",
    "numWords = len(allText[0])\n",
    "classOccurrences = np.zeros([numWords, 2])\n",
    "# every row of classOccurrences => [numRealArticles, numFakeArticles]\n",
    "\n",
    "# for every article, update class occurrences\n",
    "for i in range(len(trainFrame)):\n",
    "    row = trainFrame.iloc[i]\n",
    "    rowClass = row['Label']\n",
    "    # vectorise article text\n",
    "    rowVector = hotCodeWordArray(row['Text'], allText[0])\n",
    "    print(\"Training from row %d\" % i)\n",
    "    \n",
    "    # for every seen word\n",
    "    for j in range(len(allText[0])):\n",
    "        # if word j occurs in the article increment class occurences\n",
    "        if rowVector[j] == True:\n",
    "            if rowClass == 0:\n",
    "                # real article\n",
    "                classOccurrences[j][0] += 1\n",
    "            else:\n",
    "                # fake article\n",
    "                classOccurrences[j][1] += 1\n",
    "                \n",
    "\n",
    "                \n",
    "print(classOccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
