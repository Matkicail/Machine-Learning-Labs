{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dask import delayed\n",
    "import numpy as np\n",
    "import math\n",
    "from time import sleep\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "##Imports the song playing capacity.\n",
    "import webbrowser\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from sklearn import svm\n",
    "\n",
    "#Philani's imports\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "\n",
    "init_notebook_mode(connected=True)   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello To My Friends\n",
    "\n",
    "### My Notes\n",
    "The program requires the sleep statement this is just to ensure that I can see that certain parts are actually functioning in parallel. <br>\n",
    "The idea here is that pandas is excellent at reading in csv files so keep pandas to read. <br>\n",
    "Pandas also does let you do mysql-esque queries so keep it for the same reason. <br>\n",
    "Pandas has a good data display program - find that Matthew in the other failed itterations <br>\n",
    "The string lib is for the translate <br>\n",
    "The Math lib is to find NaNs <br>\n",
    "The Random lib is just to get samples (training....) <br>\n",
    "The Collections given the itteration could be one of two things: 1 is it is me actually using the counters. <br>\n",
    "The other which is 2: I will swap over to two sets of arrays. I dont think that this might be a smart idea but I am likely to try it <br>\n",
    "There is music just to serve as an alarm for when the data set is done, I thought to add things that are what you do not listen to so that you will always be able to tell when the program is done due to that, however should you want to turn it off feel absolutely free, just remove its library or the line in code where it occurs.<br>\n",
    "It is in good faith as an artistic expression of how to tell when the program is done. But remove it when you feel the program works with your algorithm. Please try to segment your codes using a header similar to mine.\n",
    "### How\n",
    "loadData give it the name of the file you want, the names are tailored to the csv file of fake news <br>\n",
    "\n",
    "removeStringWords takes in the string(since here we except to give it a title or the text) and then it attempts to remove the naughty words from that string, this is a submethod in the cleanString method. Please if you can actually force it to delete all strings that are bad words, I will thank you cause it seems to not always want to. You can test this and you will see that sometimes the removals make certain strings unreadable - there is some Elton John title in there about sharks and paintings on airplanes ??? I dont know what is going on but that is all it said <br>\n",
    "\n",
    "CleanString takes in a string for cleaning, it makes them all lowercase, it __attempts__ to remove all punctuation in it using the translate method. It then splits on spaces and calls the cleanString.<br>\n",
    "\n",
    "Create data sets just gets the data that you want to make your valid, train and test sets from, it uses a panda dataframe so always just pass it that variable from the loadData given your csv file name. Percentage in it is basically the percent reserved for testing and training - split equally i.e give it x and each gets x/2. <br>\n",
    "\n",
    "CreateIndices I would not worry about, but it just chooses randomly which indices are reserved for testing, validation and training<br>\n",
    "\n",
    "getCounters is me attempting __*current Matthew*__ to try and get the counters in some fast parallelized way, since the data calculates quite quickly on just this small set, actually delaying(check dask delayed) makes it slower, it is just me trying to test it works. Please do not leave sleep statements anywhere, if I am done and I have not removed that import and I gave it to you, remove it and kill the sleeps. You will have slow data otherwise<br>\n",
    "\n",
    "Ignore appendArr unless you are really interested, but that is kind of the reason why we will need to find someway to print our parallel visualize somewhere else probably as an image on a doc or something but to tell you I don't know what it is doing would be an understatement. <br>\n",
    "\n",
    "Add Counters is similar since this works with appendArr. <br>\n",
    "\n",
    "tempNameCounter just gets the counters for the relevant title and data frame given the labels possible and those that you are interested in. <br>\n",
    "\n",
    "getRelevantInfo is an abstraction of the functions so that you dont need to worry about the rest only that you need to worry about the outputs for the rest. It basically gets the counters for each label and returns them in order for the labels you gave it i.e if I said labels = [a,b,c] where abc are some arbitrary label then it would return them in that order for the labels of a, b and c. It specifically splits pandas as you know how that would work given reading the above. <br>\n",
    "\n",
    "DataWarning is me warning you of how long things will take to compute roughly, it depends a bit on your cores but not as much as I would like to given that counters is the real problem. I thought about - maybe I said it somewhere in here - doing two arrays side by side and looking at the strings, finding their index and then incrementing at that index as I read through but I have not done that. I could do it but it is a bit weird and I dont know how much it would impact counters since there would still be a lot of resizing and counters due to the fact *I believe* is a dictionary and that python treats dictionaries as hash tables, it is already quite good and should be similar to the method I would like to do. <br>\n",
    "\n",
    "Note that however, the get functions return for whatever dataFrame you called given that respective get, the data under that get along with the labels in order (so in our case a getTitle would get you the title column and then all the labels under that title that you wanted given the getRelevantInfo. I have intentionally not let you put labels into it at current given the fact that we only have labels [0,1] <br>\n",
    "\n",
    "Last addition was an unzip function, give it the data you get from your get function so if you getTitles give it that getTitles function. I recommend that what you do is you pass it something like unzip(getTitles(*dataFrame*)) where that is the dataframe you want and are interested in. But you can use any of the three gets. It will return to you a matrix (so just take from it the specific parts you are interested in e.g temp = unzip(getTitles(trainFrame)), I want the words that are real so I say temp[0] to get the titles that are true. temp[1] to get the count of those that are true __*IN ORDER as they appear in temp[0]*__ then temp[2] gives fake news titles and temp[3] their respective count. That should be all that is necessary.\n",
    "\n",
    "### Notes on the Data Set\n",
    "The CSV file contains the news articles. The articles are in the order of - ID, Title, Author, Text, Label. <br>\n",
    "There are other languages in the data set, there is French and Russian at the very least. These will make calculations take a bit longer since more entries.<br>\n",
    "We should attempt to only keep the top ~10 000 words or so, otherwise the calculation could be a nightmare.<br>\n",
    "We could have preprocessed it in visual studio however, I hope that at some point we can just get it to do a web scrape which can boost the mark a bit, what I am referring to is that we can after calculating all our probabilities we just store that (for our best itteration or some random itteration). Then someone would just uses the machine learning algorithm, and it allows a user to enter a link for a news site. Then it attempts to tell if the article is fake news or fake news.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    return pd.read_csv(name, names = ['ID','Title','Author','Text','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStringWords(string, badWords):\n",
    "    size = len(badWords)\n",
    "    for i in range(size):\n",
    "        try:\n",
    "            string.remove(badWords[i])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(text):\n",
    "    badWords = ['not','you','at','from','of','us','in','have','yes','no','are','','for','but','that','it','this','he','she','they','that','a','an','who','where','there','his','her','their','i','my','we','our','were','the','if','as','and','in','on','we','to','also','so','is','its']\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.split(' ')\n",
    "        text = removeStringWords(text, badWords)\n",
    "        #rint('success')\n",
    "        #sleep(0.01)\n",
    "        return text\n",
    "    except:\n",
    "        #print('fail')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSets(totData, percent):\n",
    "    dataSize = len(totData)\n",
    "    testIndices, validIndices = createIndices(dataSize, percent)\n",
    "    trainIndices = np.empty(0)\n",
    "    for i in range(dataSize):\n",
    "        \n",
    "        if i not in testIndices and i not in validIndices:\n",
    "            trainIndices = np.append(trainIndices, i)\n",
    "            \n",
    "    testFrame = totData.drop(trainIndices)\n",
    "    testFrame = testFrame.drop(validIndices)\n",
    "    validFrame = totData.drop(trainIndices)\n",
    "    validFrame = validFrame.drop(testIndices)\n",
    "    trainFrame = totData.drop(testIndices)\n",
    "    trainFrame = trainFrame.drop(validIndices)\n",
    "    return trainFrame, testFrame, validFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndices(dataSize, percent):\n",
    "    sizeChoice = int(dataSize * 0.2)\n",
    "    randomChoices = random.sample(range(dataSize-1),sizeChoice)\n",
    "    half = int(sizeChoice/2)\n",
    "    firstHalf = randomChoices[0:half]\n",
    "    secondHalf = randomChoices[half:]\n",
    "    return firstHalf, secondHalf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that this is given the data, the title of the dataframe and then the labels for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounters(data, title, labelTitle, labelDesired):\n",
    "        dataTitle = data.loc[data[labelTitle] == labelDesired]\n",
    "        titleArray = dataTitle[title].to_numpy()\n",
    "        results = []\n",
    "        for i in titleArray:\n",
    "            y = delayed(cleanString)(i)\n",
    "            try:\n",
    "                p = float(y[0])\n",
    "                pass\n",
    "            except:\n",
    "                if y is not None:\n",
    "                    #print(y)\n",
    "                    appendArr(results,y)\n",
    "                \n",
    "        texts = delayed(results)\n",
    "        return texts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArr(text, y):\n",
    "    return text.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCounters(prevCount, currCount):\n",
    "    return prevCount + currCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempNameCounter(dataFrame, title, labelTitle, labelDesired):\n",
    "    titleCounterReal = getCounters(trainFrame, title, labelTitle, labelDesired)\n",
    "    results = []\n",
    "    resultstwo = []\n",
    "    j = 0\n",
    "    for i in titleCounterReal:\n",
    "        y = delayed(count_words)(i)\n",
    "        if j % 2:\n",
    "            results.append(y)\n",
    "        else:\n",
    "            resultstwo.append(y) \n",
    "    bigCount = delayed(addCounters)(results,resultstwo)\n",
    "    done = bigCount.compute() \n",
    "    #bigCount.visualize()\n",
    "    sadMe = Counter()\n",
    "    for i in done:\n",
    "        sadMe = sadMe + count_words(i)\n",
    "    return sadMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantInfo(dataFrame, title, labelTitle, labels):\n",
    "    results = []\n",
    "    j = 0\n",
    "    for i in labels:\n",
    "        #print(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        results.append(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        print(\"result at index \" + str(j) + \" corresponds to \" + title + \" of label \" + str(i))\n",
    "        j +=1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataWarning(name):\n",
    "    if name == 'tenPercent.csv':\n",
    "        print(\"Expect a short load time, stay around\")\n",
    "        return\n",
    "    elif name == 'twentyPercent.csv':\n",
    "        print('Expect a low load time, stay around')\n",
    "    elif name == 'fourtyPercent.csv':\n",
    "        print('Expect a few minites, check your phone or something')\n",
    "    elif name == 'eightyPercent.csv':\n",
    "        print('Expect a relatively long load time, do something else in the mean while')\n",
    "    else:\n",
    "        print('Expect a long load time, make a cup of tea or something...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitles(dataFrame):\n",
    "    print(\"Running Titles\")\n",
    "    return  getRelevantInfo(dataFrame, 'Title', 'Label', [0,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Text', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthor(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Author', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(data):\n",
    "    wordsReal, numberReal = [list(c) for c in zip(*list(data[0].items()))]\n",
    "    wordsFake, numberFake = [list(c) for c in zip(*list(data[1].items()))]\n",
    "    return [wordsReal,numberReal,wordsFake,numberFake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playSongWhenDone():\n",
    "    # feel free to add songs or remove them as you want, just follow the format of nextNum,songLink\n",
    "    temp = pd.read_csv('songs.csv', names = ['id', 'url'])\n",
    "    ID = temp['id'].to_numpy()\n",
    "    urls = temp['url'].to_numpy()\n",
    "    i = random.sample(range(len(ID)),1)\n",
    "    song = urls[i]\n",
    "    try:\n",
    "        webbrowser.open(song[0])\n",
    "    except:\n",
    "        print(\"no internet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume that we pass this the results from someething like getText\n",
    "#This implies that result[0] = realWords, results[2] = fake words\n",
    "#mainly done in case you get new words and need to make some form laplace smoothing to see if the words are contained in your databased for a given set of data\n",
    "#e.g have my titles ever seen this word\n",
    "def createBinaryAllDataString(results):\n",
    "    #use the longest array\n",
    "    newArray = np.empty(0)\n",
    "    usedReal = True\n",
    "    if len(results[0]) > len(results[1]):\n",
    "        newArray = results[0]\n",
    "        usedReal = True\n",
    "    else:\n",
    "        newArray = results[2]\n",
    "        usedReal = False\n",
    "    if usedReal == True:\n",
    "        for i in results[2]:\n",
    "            if i not in newArray:\n",
    "                newArray = np.append(newArray, i)\n",
    "    else:\n",
    "        for i in results[0]:\n",
    "            if i not in newArray:\n",
    "                newArray = np.append(newArray, i)\n",
    "   # testCreateBinaryDataStrings(results[0], results[2], newArray)\n",
    "    return newArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please only use this to test to see if it generates the binary data correctly it is referenced in createBinaryDataStrings.\n",
    "#The idea is that should we have all the words captured the result array since it is constituted of words should be equal to the length of the the two sets of words at it's max but obviously\n",
    "#we expect it to be a bit or a lot smaller than it, so I made a way of testing it to see that everytime it finds a word it counts it as a 1(for being true in the array)\n",
    "#if the printed counter is not the same as teh array in size then clearly something must be missing.\n",
    "#hence why this will definitely show that the above works\n",
    "def testCreateBinaryDataStrings(realArray, fakeArray, resultArray):\n",
    "    print(len(realArray))\n",
    "    print(len(fakeArray))\n",
    "    j = 0 \n",
    "    for i in realArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    j = 0\n",
    "    for i in fakeArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    print(len(resultArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriseData(dataPoint, allData):\n",
    "    size = len(allData)\n",
    "    binary = np.zeros(size)\n",
    "    for j in dataPoint:\n",
    "                         \n",
    "        if j in allData:\n",
    "            print(j)          \n",
    "            binary[np.where(allData==j)] = 1\n",
    "                         \n",
    "    return binary\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingModel(allData, num_dimensions_for_model):\n",
    "    #Use all words in titles and texts to train embedding model\n",
    "    allTitles = allData[:,1]\n",
    "    allText = allData[:,3]\n",
    "    allTitlesAndText = np.concatenate((allText,allTitles))\n",
    "        \n",
    "    dataForEmbedding = []\n",
    "    \n",
    "    remove = string.punctuation\n",
    "    \n",
    "    for currTitleOrText in allTitlesAndText:\n",
    "        #For each title or text convert to sentences and words in order to\n",
    "        #get the data in the correct format to train the embedding model\n",
    "\n",
    "        for j in sent_tokenize(currTitleOrText):\n",
    "                \n",
    "            temp = [] \n",
    "      \n",
    "            # tokenize the sentence into words \n",
    "            for k in word_tokenize(j):\n",
    "                \n",
    "                curr_word = k + ''\n",
    "                #remove all punctuation from word\n",
    "                curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "                \n",
    "                if len(curr_word) > 0:\n",
    "                    temp.append(curr_word.lower()) \n",
    "  \n",
    "            if len(temp) > 0:\n",
    "                dataForEmbedding.append(temp) \n",
    "            \n",
    "    \n",
    "    embeddingModel = gensim.models.Word2Vec(dataForEmbedding, min_count = 1,  size = num_dimensions_for_model, window = 5)\n",
    "    \n",
    "    return embeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanedData(data):\n",
    "    #Remove Nan, remove punctuation, and new lines\n",
    "    feature_names = np.array(['ID','Title','Author','Text'])\n",
    "    \n",
    "    #Convert NaNs\n",
    "    for i in range(3):\n",
    "        string_replacement = \"\"\n",
    "        if i==1:\n",
    "            string_replacement = \"-NO AUTHOR-\"\n",
    "        else:\n",
    "            string_replacement = \"NaN\"\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            if pd.isnull(data[j][i+1]):\n",
    "                data[j][2] = string_replacement\n",
    "        \n",
    "    #Defining which punctuation to remove\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\".\",\"“\")\n",
    "    remove = remove.replace(\"!\",\"”\")\n",
    "    remove = remove.replace(\"?\",\"’\")\n",
    "    remove = remove + '‘'\n",
    "    remove = remove + '—'\n",
    "    remove = remove + '–'\n",
    "\n",
    "    #Remove NaNs\n",
    "    data = data[np.all(data != \"NaN\", axis = 1)]\n",
    "    \n",
    "    #Remove punctuation(except '.','?','!') and new lines\n",
    "    for i in range(3):\n",
    "        for j in range(len(data)):\n",
    "            data[j][i+1] = data[j][i+1].replace(\"\\n\",\"\").translate(str.maketrans('', '', remove))\n",
    "    \n",
    "    return feature_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowsToDataPoints(data, embedding_model, model_vocab, num_expected_columns):\n",
    "    #Convert all rows in data to relevant data_points\n",
    "    data_points_with_labels_list = []\n",
    "    for row in data:\n",
    "        convertedRow = convertRowToDataPoint(row, embedding_model, model_vocab)\n",
    "        if len(convertedRow) == num_expected_columns:\n",
    "            data_points_with_labels_list.append(convertRowToDataPoint(row, embedding_model, model_vocab))\n",
    "        \n",
    "    data_points_with_labels = np.array(data_points_with_labels_list)\n",
    "    return data_points_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToDataPoint(row, embedding_model, model_vocab):\n",
    "    #Take in row and convert to data point\n",
    "    #Stores text points then title points then label\n",
    "    curr_title = row[1]\n",
    "    curr_text = row[3]\n",
    "    data_point = np.array([row[4]])\n",
    "    data_point = np.append(getAverageEmbedding(curr_title, embedding_model, model_vocab), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_text, embedding_model, model_vocab), data_point)\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToDataPointWithoutLabel(row, embedding_model, model_vocab):\n",
    "    #Take in row and convert to data point\n",
    "    #Stores text points then title points then label\n",
    "    curr_title = row[1]\n",
    "    curr_text = row[3]\n",
    "    data_point = np.array([])\n",
    "    data_point = np.append(getAverageEmbedding(curr_title, embedding_model, model_vocab), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_text, embedding_model, model_vocab), data_point)\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageEmbedding(curr_string, embedding_model, model_vocab):\n",
    "    #Take in string that has already been cleaned of punctuation and newlines\n",
    "    total_words = 0.0\n",
    "    stop_words = ['not','you','at','from','of','us','in','have','yes','no','are','','for','but','that','it','this','he','she','they','that','a','an','who','where','there','his','her','their','i','my','we','our','were','the','if','as','and','in','on','we','to','also','so','is','its']\n",
    "    remove = string.punctuation\n",
    "    curr_embedding = np.array([])\n",
    "    for k in word_tokenize(curr_string):\n",
    "        curr_word = k + ''\n",
    "        #remove all punctuation from word\n",
    "        curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "        #convert to lowercase\n",
    "        curr_word = curr_word.lower()\n",
    "        #check if it is a stop word or empty\n",
    "        if curr_word in stop_words:\n",
    "            continue\n",
    "        #check if word is in model_vocab\n",
    "        if curr_word not in model_vocab:\n",
    "            continue\n",
    "        \n",
    "        if len(curr_embedding) == 0:\n",
    "            curr_embedding = embedding_model[curr_word]\n",
    "        else:\n",
    "            curr_embedding = curr_embedding+embedding_model[curr_word]\n",
    "        \n",
    "        total_words = total_words + 1.0\n",
    "    \n",
    "    curr_embedding = curr_embedding/total_words\n",
    "    return curr_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset (start off with a small set, then work your way up). \n",
    "# Convert all data to numpy array, because they are faster and easier to work with.\n",
    "name = 'Actual Data.csv'\n",
    "data = loadData(name)\n",
    "data = data.to_numpy()\n",
    "# print(data[:5])\n",
    "\n",
    "# Clean the data so that it can be used in training (This refers to removing rows that contain NaN, removing punctuation, and removing \\n(newlines))\n",
    "# Use cleaned data to train our embedding model. The embedding model is what is used to convert the words to meaningful vectors. \n",
    "# I give the function the data and the number of dimensions I want my vector to be after converting the word.\n",
    "feature_names, data = getCleanedData(data)\n",
    "EmbeddingModel = getEmbeddingModel(data, 100) # test\n",
    "model_vocab = EmbeddingModel.wv.vocab # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demonstration of converting our data to a usable format(i.e from words to vectors)\n",
    "trainData = data\n",
    "trainData2 = data\n",
    "#validationData = data[100:150]\n",
    "\n",
    "# Send data to method in order to convert it. Passed through the data, the embedding model we've previously trained,\n",
    "# the vocabulary of words in our embedded model(to check if we can actually convert a word to a vector or not), \n",
    "# and the number of columns we expect in the matrix that will be returned(at the moment I am calculating this\n",
    "# by using the formula 2*num_dimensions_used_in_embedding + 1).\n",
    "trainData = convertRowsToDataPoints(trainData, EmbeddingModel, model_vocab, 201)\n",
    "trainData2 = convertRowsToDataPoints(trainData2, EmbeddingModel, model_vocab, 201)\n",
    "#validationData = convertRowsToDataPoints(validationData, EmbeddingModel, model_vocab, 201)\n",
    "\n",
    "#Split y_labels from data to store separately\n",
    "trainData_y = trainData[:,200]\n",
    "trainData2_y = trainData2[:,200]\n",
    "\n",
    "#validationData_y = validationData[:,200]\n",
    "\n",
    "#Delete y_labels from data\n",
    "trainData = np.delete(trainData, 200, 1)\n",
    "trainData2 = np.delete(trainData2, 200, 1)\n",
    "#validationData = np.delete(validationData, 200, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation\n",
    "\n",
    "Logistic regression will be accomplished using two implementations of gradient descent: loss minimization and the maximum likelihood estimation. \n",
    "\n",
    "Reference to the website that helped a lot with the functions:\n",
    "\n",
    "Kaggle.com. 2020. Logistic Regression From Scratch - Python. [online] Available at: <https://www.kaggle.com/jeppbautista/logistic-regression-from-scratch-python> [Accessed 17 April 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, weight):\n",
    "    z = np.dot(X, weight)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# The following functions will be used in the implementation of loss minimizing with the gradient descent.\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "def gradient_descent(X, h, y):\n",
    "    return np.dot(X.T, (h - y)) / y.shape[0]\n",
    "\n",
    "def update_weight_loss(weight, learning_rate, gradient):\n",
    "    return weight - learning_rate * gradient\n",
    "\n",
    "#The following functions will be used in the implementation of the maximum likelihood estimation with gradient ascent.\n",
    "def log_likelihood(x, y, weights):\n",
    "    z = np.dot(x, weights)\n",
    "    ll = np.sum( y*z - np.log(1 + np.exp(z)) )\n",
    "    return ll\n",
    "\n",
    "def gradient_ascent(X, h, y):\n",
    "    return np.dot(X.T, y - h)\n",
    "\n",
    "def update_weight_mle(weight, learning_rate, gradient):\n",
    "    return weight + learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (Log Reg using Gradient descent):565.4274728298187 seconds\n",
      "Learning rate: 0.5\n",
      "Iteration: 100000\n"
     ]
    }
   ],
   "source": [
    "# Implementation of loss minimization with gradient descent:\n",
    "start_time = time.time()\n",
    "num_iter = 100000\n",
    "\n",
    "intercept = np.ones((trainData.shape[0], 1)) \n",
    "trainData = np.concatenate((intercept, trainData), axis=1)\n",
    "theta = np.zeros(trainData.shape[1])\n",
    "\n",
    "for i in range(num_iter):\n",
    "    h = sigmoid(trainData, theta)\n",
    "    gradient = gradient_descent(trainData, h, trainData_y)\n",
    "    theta = update_weight_loss(theta, 0.5, gradient)\n",
    "    \n",
    "print(\"Training time (Log Reg using Gradient descent):\" + str(time.time() - start_time) + \" seconds\")\n",
    "print(\"Learning rate: {}\\nIteration: {}\".format(0.5, num_iter))\n",
    "\n",
    "# Illustrations of loss minimization with gradient descent:\n",
    "result = sigmoid(trainData, theta)\n",
    "f = pd.DataFrame(np.around(result, decimals=6)) \n",
    "f['Label'] = trainData_y\n",
    "f['Classifier'] = f[0].apply(lambda x : 0 if x < 0.5 else 1)\n",
    "f.columns = ['Value','Label','Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Label</th>\n",
       "      <th>Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.944316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.997359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.765801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20119</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20120</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20121</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20122</td>\n",
       "      <td>0.997090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20123</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20124 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Value  Label  Classifier\n",
       "0      0.944316    1.0           1\n",
       "1      0.411916    0.0           0\n",
       "2      0.997359    1.0           1\n",
       "3      0.844300    1.0           1\n",
       "4      0.765801    1.0           1\n",
       "...         ...    ...         ...\n",
       "20119  0.027655    0.0           0\n",
       "20120  0.000015    0.0           0\n",
       "20121  0.001544    0.0           0\n",
       "20122  0.997090    1.0           1\n",
       "20123  0.998886    1.0           1\n",
       "\n",
       "[20124 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function of the implementation of loss minimization with gradient descent:\n",
    "result.sort()\n",
    "x = np.zeros(trainData.shape[0])\n",
    "for i in range(trainData.shape[0]):\n",
    "    x[i] = i\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(z)\")\n",
    "plt.plot(x,result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[9772  614]\n",
      " [ 438 9300]]\n",
      "Accuracy Score : 0.9477241105148082\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.94      0.95     10386\n",
      "         1.0       0.94      0.96      0.95      9738\n",
      "\n",
      "    accuracy                           0.95     20124\n",
      "   macro avg       0.95      0.95      0.95     20124\n",
      "weighted avg       0.95      0.95      0.95     20124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the implementation of loss minimization with gradient descent:\n",
    "# (Thanks to the help of the scikit-learn library).\n",
    "actual = f['Label']\n",
    "predicted = f['Classifier']\n",
    "cf = confusion_matrix(actual, predicted)\n",
    "\n",
    "print('Confusion Matrix :')\n",
    "print(cf) \n",
    "print('Accuracy Score :',accuracy_score(actual, predicted)) \n",
    "print('Report : ')\n",
    "print(classification_report(actual, predicted)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (Log Reg using MLE):1353.0205280780792seconds\n",
      "Learning rate: 0.5\n",
      "Iteration: 100000\n"
     ]
    }
   ],
   "source": [
    "# Implementation of the maximum likelihood estimation with gradient ascent: \n",
    "start_time = time.time()\n",
    "num_iter = 100000\n",
    "\n",
    "intercept2 = np.ones((trainData2.shape[0], 1))\n",
    "trainData2 = np.concatenate((intercept2, trainData2), axis=1)\n",
    "theta2 = np.zeros(trainData2.shape[1])\n",
    "\n",
    "for i in range(num_iter):\n",
    "    h2 = sigmoid(trainData2, theta2)\n",
    "    gradient2 = gradient_ascent(trainData2, h2, trainData2_y)\n",
    "    theta2 = update_weight_mle(theta2, 0.5, gradient2)\n",
    "    \n",
    "print(\"Training time (Log Reg using MLE):\" + str(time.time() - start_time) + \"seconds\")\n",
    "print(\"Learning rate: {}\\nIteration: {}\".format(0.5, num_iter))\n",
    "\n",
    "# Illustrations of the maximum likelihood estimation with gradient ascent:\n",
    "result2 = sigmoid(trainData2, theta2)\n",
    "f2 = pd.DataFrame(np.around(result2, decimals=6))\n",
    "f2['Label'] = trainData2_y\n",
    "f2['Classifier'] = f2[0].apply(lambda x : 0 if x < 0.5 else 1)\n",
    "f2.columns = ['Value','Label','Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Label</th>\n",
       "      <th>Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20124 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Value  Label  Classifier\n",
       "0        1.0    1.0           1\n",
       "1        0.0    0.0           0\n",
       "2        1.0    1.0           1\n",
       "3        1.0    1.0           1\n",
       "4        1.0    1.0           1\n",
       "...      ...    ...         ...\n",
       "20119    0.0    0.0           0\n",
       "20120    0.0    0.0           0\n",
       "20121    0.0    0.0           0\n",
       "20122    1.0    1.0           1\n",
       "20123    1.0    1.0           1\n",
       "\n",
       "[20124 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function of the implementation of the maximum likelihood estimation with gradient descent:\n",
    "result2.sort()\n",
    "x = np.zeros(trainData.shape[0])\n",
    "for i in range(trainData.shape[0]):\n",
    "    x[i] = i\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(z)\")\n",
    "plt.plot(x,result2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[9808  578]\n",
      " [ 540 9198]]\n",
      "Accuracy Score : 0.9444444444444444\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.94      0.95     10386\n",
      "         1.0       0.94      0.94      0.94      9738\n",
      "\n",
      "    accuracy                           0.94     20124\n",
      "   macro avg       0.94      0.94      0.94     20124\n",
      "weighted avg       0.94      0.94      0.94     20124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the implementation of the maximum likelihood estimation with gradient ascent:\n",
    "# (Thanks to the help of the scikit-learn library).\n",
    "\n",
    "actual2 = f2['Label']\n",
    "predicted2 = f2['Classifier']\n",
    "cf2 = confusion_matrix(actual2, predicted2)\n",
    "\n",
    "print('Confusion Matrix :')\n",
    "print(cf2) \n",
    "print('Accuracy Score :',accuracy_score(actual2, predicted2)) \n",
    "print('Report : ')\n",
    "print(classification_report(actual2, predicted2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
