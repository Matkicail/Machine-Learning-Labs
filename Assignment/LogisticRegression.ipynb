{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dask import delayed\n",
    "import numpy as np\n",
    "import math\n",
    "from time import sleep\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "##Imports the song playing capacity.\n",
    "import webbrowser\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from sklearn import svm\n",
    "\n",
    "#Philani's imports\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "\n",
    "init_notebook_mode(connected=True)   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello To My Friends\n",
    "\n",
    "### My Notes\n",
    "The program requires the sleep statement this is just to ensure that I can see that certain parts are actually functioning in parallel. <br>\n",
    "The idea here is that pandas is excellent at reading in csv files so keep pandas to read. <br>\n",
    "Pandas also does let you do mysql-esque queries so keep it for the same reason. <br>\n",
    "Pandas has a good data display program - find that Matthew in the other failed itterations <br>\n",
    "The string lib is for the translate <br>\n",
    "The Math lib is to find NaNs <br>\n",
    "The Random lib is just to get samples (training....) <br>\n",
    "The Collections given the itteration could be one of two things: 1 is it is me actually using the counters. <br>\n",
    "The other which is 2: I will swap over to two sets of arrays. I dont think that this might be a smart idea but I am likely to try it <br>\n",
    "There is music just to serve as an alarm for when the data set is done, I thought to add things that are what you do not listen to so that you will always be able to tell when the program is done due to that, however should you want to turn it off feel absolutely free, just remove its library or the line in code where it occurs.<br>\n",
    "It is in good faith as an artistic expression of how to tell when the program is done. But remove it when you feel the program works with your algorithm. Please try to segment your codes using a header similar to mine.\n",
    "### How\n",
    "loadData give it the name of the file you want, the names are tailored to the csv file of fake news <br>\n",
    "\n",
    "removeStringWords takes in the string(since here we except to give it a title or the text) and then it attempts to remove the naughty words from that string, this is a submethod in the cleanString method. Please if you can actually force it to delete all strings that are bad words, I will thank you cause it seems to not always want to. You can test this and you will see that sometimes the removals make certain strings unreadable - there is some Elton John title in there about sharks and paintings on airplanes ??? I dont know what is going on but that is all it said <br>\n",
    "\n",
    "CleanString takes in a string for cleaning, it makes them all lowercase, it __attempts__ to remove all punctuation in it using the translate method. It then splits on spaces and calls the cleanString.<br>\n",
    "\n",
    "Create data sets just gets the data that you want to make your valid, train and test sets from, it uses a panda dataframe so always just pass it that variable from the loadData given your csv file name. Percentage in it is basically the percent reserved for testing and training - split equally i.e give it x and each gets x/2. <br>\n",
    "\n",
    "CreateIndices I would not worry about, but it just chooses randomly which indices are reserved for testing, validation and training<br>\n",
    "\n",
    "getCounters is me attempting __*current Matthew*__ to try and get the counters in some fast parallelized way, since the data calculates quite quickly on just this small set, actually delaying(check dask delayed) makes it slower, it is just me trying to test it works. Please do not leave sleep statements anywhere, if I am done and I have not removed that import and I gave it to you, remove it and kill the sleeps. You will have slow data otherwise<br>\n",
    "\n",
    "Ignore appendArr unless you are really interested, but that is kind of the reason why we will need to find someway to print our parallel visualize somewhere else probably as an image on a doc or something but to tell you I don't know what it is doing would be an understatement. <br>\n",
    "\n",
    "Add Counters is similar since this works with appendArr. <br>\n",
    "\n",
    "tempNameCounter just gets the counters for the relevant title and data frame given the labels possible and those that you are interested in. <br>\n",
    "\n",
    "getRelevantInfo is an abstraction of the functions so that you dont need to worry about the rest only that you need to worry about the outputs for the rest. It basically gets the counters for each label and returns them in order for the labels you gave it i.e if I said labels = [a,b,c] where abc are some arbitrary label then it would return them in that order for the labels of a, b and c. It specifically splits pandas as you know how that would work given reading the above. <br>\n",
    "\n",
    "DataWarning is me warning you of how long things will take to compute roughly, it depends a bit on your cores but not as much as I would like to given that counters is the real problem. I thought about - maybe I said it somewhere in here - doing two arrays side by side and looking at the strings, finding their index and then incrementing at that index as I read through but I have not done that. I could do it but it is a bit weird and I dont know how much it would impact counters since there would still be a lot of resizing and counters due to the fact *I believe* is a dictionary and that python treats dictionaries as hash tables, it is already quite good and should be similar to the method I would like to do. <br>\n",
    "\n",
    "Note that however, the get functions return for whatever dataFrame you called given that respective get, the data under that get along with the labels in order (so in our case a getTitle would get you the title column and then all the labels under that title that you wanted given the getRelevantInfo. I have intentionally not let you put labels into it at current given the fact that we only have labels [0,1] <br>\n",
    "\n",
    "Last addition was an unzip function, give it the data you get from your get function so if you getTitles give it that getTitles function. I recommend that what you do is you pass it something like unzip(getTitles(*dataFrame*)) where that is the dataframe you want and are interested in. But you can use any of the three gets. It will return to you a matrix (so just take from it the specific parts you are interested in e.g temp = unzip(getTitles(trainFrame)), I want the words that are real so I say temp[0] to get the titles that are true. temp[1] to get the count of those that are true __*IN ORDER as they appear in temp[0]*__ then temp[2] gives fake news titles and temp[3] their respective count. That should be all that is necessary.\n",
    "\n",
    "### Notes on the Data Set\n",
    "The CSV file contains the news articles. The articles are in the order of - ID, Title, Author, Text, Label. <br>\n",
    "There are other languages in the data set, there is French and Russian at the very least. These will make calculations take a bit longer since more entries.<br>\n",
    "We should attempt to only keep the top ~10 000 words or so, otherwise the calculation could be a nightmare.<br>\n",
    "We could have preprocessed it in visual studio however, I hope that at some point we can just get it to do a web scrape which can boost the mark a bit, what I am referring to is that we can after calculating all our probabilities we just store that (for our best itteration or some random itteration). Then someone would just uses the machine learning algorithm, and it allows a user to enter a link for a news site. Then it attempts to tell if the article is fake news or fake news.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    return pd.read_csv(name, names = ['ID','Title','Author','Text','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStringWords(string, badWords):\n",
    "    size = len(badWords)\n",
    "    for i in range(size):\n",
    "        try:\n",
    "            string.remove(badWords[i])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(text):\n",
    "    badWords = ['not','you','at','from','of','us','in','have','yes','no','are','','for','but','that','it','this','he','she','they','that','a','an','who','where','there','his','her','their','i','my','we','our','were','the','if','as','and','in','on','we','to','also','so','is','its']\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.split(' ')\n",
    "        text = removeStringWords(text, badWords)\n",
    "        #rint('success')\n",
    "        #sleep(0.01)\n",
    "        return text\n",
    "    except:\n",
    "        #print('fail')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSets(totData, percent):\n",
    "    dataSize = len(totData)\n",
    "    testIndices, validIndices = createIndices(dataSize, percent)\n",
    "    trainIndices = np.empty(0)\n",
    "    for i in range(dataSize):\n",
    "        \n",
    "        if i not in testIndices and i not in validIndices:\n",
    "            trainIndices = np.append(trainIndices, i)\n",
    "            \n",
    "    testFrame = totData.drop(trainIndices)\n",
    "    testFrame = testFrame.drop(validIndices)\n",
    "    validFrame = totData.drop(trainIndices)\n",
    "    validFrame = validFrame.drop(testIndices)\n",
    "    trainFrame = totData.drop(testIndices)\n",
    "    trainFrame = trainFrame.drop(validIndices)\n",
    "    return trainFrame, testFrame, validFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIndices(dataSize, percent):\n",
    "    sizeChoice = int(dataSize * 0.2)\n",
    "    randomChoices = random.sample(range(dataSize-1),sizeChoice)\n",
    "    half = int(sizeChoice/2)\n",
    "    firstHalf = randomChoices[0:half]\n",
    "    secondHalf = randomChoices[half:]\n",
    "    return firstHalf, secondHalf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that this is given the data, the title of the dataframe and then the labels for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounters(data, title, labelTitle, labelDesired):\n",
    "        dataTitle = data.loc[data[labelTitle] == labelDesired]\n",
    "        titleArray = dataTitle[title].to_numpy()\n",
    "        results = []\n",
    "        for i in titleArray:\n",
    "            y = delayed(cleanString)(i)\n",
    "            try:\n",
    "                p = float(y[0])\n",
    "                pass\n",
    "            except:\n",
    "                if y is not None:\n",
    "                    #print(y)\n",
    "                    appendArr(results,y)\n",
    "                \n",
    "        texts = delayed(results)\n",
    "        return texts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendArr(text, y):\n",
    "    return text.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCounters(prevCount, currCount):\n",
    "    return prevCount + currCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempNameCounter(dataFrame, title, labelTitle, labelDesired):\n",
    "    titleCounterReal = getCounters(trainFrame, title, labelTitle, labelDesired)\n",
    "    results = []\n",
    "    resultstwo = []\n",
    "    j = 0\n",
    "    for i in titleCounterReal:\n",
    "        y = delayed(count_words)(i)\n",
    "        if j % 2:\n",
    "            results.append(y)\n",
    "        else:\n",
    "            resultstwo.append(y) \n",
    "    bigCount = delayed(addCounters)(results,resultstwo)\n",
    "    done = bigCount.compute() \n",
    "    #bigCount.visualize()\n",
    "    sadMe = Counter()\n",
    "    for i in done:\n",
    "        sadMe = sadMe + count_words(i)\n",
    "    return sadMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantInfo(dataFrame, title, labelTitle, labels):\n",
    "    results = []\n",
    "    j = 0\n",
    "    for i in labels:\n",
    "        #print(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        results.append(tempNameCounter(dataFrame, title, labelTitle, i))\n",
    "        print(\"result at index \" + str(j) + \" corresponds to \" + title + \" of label \" + str(i))\n",
    "        j +=1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataWarning(name):\n",
    "    if name == 'twentyPercent.csv':\n",
    "        print(\"Expect a short load time, stay around\")\n",
    "        return\n",
    "    elif name == 'twentyPercent.csv':\n",
    "        print('Expect a low load time, stay around')\n",
    "    elif name == 'fourtyPercent.csv':\n",
    "        print('Expect a few minites, check your phone or something')\n",
    "    elif name == 'eightyPercent.csv':\n",
    "        print('Expect a relatively long load time, do something else in the mean while')\n",
    "    else:\n",
    "        print('Expect a long load time, make a cup of tea or something...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitles(dataFrame):\n",
    "    print(\"Running Titles\")\n",
    "    return  getRelevantInfo(dataFrame, 'Title', 'Label', [0,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Text', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthor(dataFrame):\n",
    "    print(\"Running Text\")\n",
    "    return getRelevantInfo(dataFrame, 'Author', 'Label', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(data):\n",
    "    wordsReal, numberReal = [list(c) for c in zip(*list(data[0].items()))]\n",
    "    wordsFake, numberFake = [list(c) for c in zip(*list(data[1].items()))]\n",
    "    return [wordsReal,numberReal,wordsFake,numberFake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playSongWhenDone():\n",
    "    # feel free to add songs or remove them as you want, just follow the format of nextNum,songLink\n",
    "    temp = pd.read_csv('songs.csv', names = ['id', 'url'])\n",
    "    ID = temp['id'].to_numpy()\n",
    "    urls = temp['url'].to_numpy()\n",
    "    i = random.sample(range(len(ID)),1)\n",
    "    song = urls[i]\n",
    "    try:\n",
    "        webbrowser.open(song[0])\n",
    "    except:\n",
    "        print(\"no internet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume that we pass this the results from someething like getText\n",
    "#This implies that result[0] = realWords, results[2] = fake words\n",
    "#mainly done in case you get new words and need to make some form laplace smoothing to see if the words are contained in your databased for a given set of data\n",
    "#e.g have my titles ever seen this word\n",
    "def createBinaryAllDataString(results):\n",
    "    #use the longest array\n",
    "    newArray = np.empty(0)\n",
    "    usedReal = True\n",
    "    if len(results[0]) > len(results[1]):\n",
    "        newArray = results[0]\n",
    "        usedReal = True\n",
    "    else:\n",
    "        newArray = results[2]\n",
    "        usedReal = False\n",
    "    if usedReal == True:\n",
    "        for i in results[2]:\n",
    "            if i not in newArray:\n",
    "                newArray = np.append(newArray, i)\n",
    "    else:\n",
    "        for i in results[0]:\n",
    "            if i not in newArray:\n",
    "                newArray = np.append(newArray, i)\n",
    "   # testCreateBinaryDataStrings(results[0], results[2], newArray)\n",
    "    return newArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please only use this to test to see if it generates the binary data correctly it is referenced in createBinaryDataStrings.\n",
    "#The idea is that should we have all the words captured the result array since it is constituted of words should be equal to the length of the the two sets of words at it's max but obviously\n",
    "#we expect it to be a bit or a lot smaller than it, so I made a way of testing it to see that everytime it finds a word it counts it as a 1(for being true in the array)\n",
    "#if the printed counter is not the same as teh array in size then clearly something must be missing.\n",
    "#hence why this will definitely show that the above works\n",
    "def testCreateBinaryDataStrings(realArray, fakeArray, resultArray):\n",
    "    print(len(realArray))\n",
    "    print(len(fakeArray))\n",
    "    j = 0 \n",
    "    for i in realArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    j = 0\n",
    "    for i in fakeArray:\n",
    "        if i in resultArray:\n",
    "            j += 1\n",
    "    print(j)\n",
    "    print(len(resultArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriseData(dataPoint, allData):\n",
    "    size = len(allData)\n",
    "    binary = np.zeros(size)\n",
    "    for j in dataPoint:\n",
    "                         \n",
    "        if j in allData:\n",
    "            print(j)          \n",
    "            binary[np.where(allData==j)] = 1\n",
    "                         \n",
    "    return binary\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingModel(allData, num_dimensions_for_model):\n",
    "    #Use all words in titles and texts to train embedding model\n",
    "    allTitles = allData[:,1]\n",
    "    allText = allData[:,3]\n",
    "    allTitlesAndText = np.concatenate((allText,allTitles))\n",
    "        \n",
    "    dataForEmbedding = []\n",
    "    \n",
    "    remove = string.punctuation\n",
    "    \n",
    "    for currTitleOrText in allTitlesAndText:\n",
    "        #For each title or text convert to sentences and words in order to\n",
    "        #get the data in the correct format to train the embedding model\n",
    "\n",
    "        for j in sent_tokenize(currTitleOrText):\n",
    "                \n",
    "            temp = [] \n",
    "      \n",
    "            # tokenize the sentence into words \n",
    "            for k in word_tokenize(j):\n",
    "                \n",
    "                curr_word = k + ''\n",
    "                #remove all punctuation from word\n",
    "                curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "                \n",
    "                if len(curr_word) > 0:\n",
    "                    temp.append(curr_word.lower()) \n",
    "  \n",
    "            if len(temp) > 0:\n",
    "                dataForEmbedding.append(temp) \n",
    "            \n",
    "    \n",
    "    embeddingModel = gensim.models.Word2Vec(dataForEmbedding, min_count = 1,  size = num_dimensions_for_model, window = 5)\n",
    "    \n",
    "    return embeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanedData(data):\n",
    "    #Remove Nan, remove punctuation, and new lines\n",
    "    feature_names = np.array(['ID','Title','Author','Text'])\n",
    "    \n",
    "    #Convert NaNs\n",
    "    for i in range(3):\n",
    "        string_replacement = \"\"\n",
    "        if i==1:\n",
    "            string_replacement = \"-NO AUTHOR-\"\n",
    "        else:\n",
    "            string_replacement = \"NaN\"\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            if pd.isnull(data[j][i+1]):\n",
    "                data[j][2] = string_replacement\n",
    "        \n",
    "    #Defining which punctuation to remove\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\".\",\"“\")\n",
    "    remove = remove.replace(\"!\",\"”\")\n",
    "    remove = remove.replace(\"?\",\"’\")\n",
    "    remove = remove + '‘'\n",
    "    remove = remove + '—'\n",
    "    remove = remove + '–'\n",
    "\n",
    "    #Remove NaNs\n",
    "    data = data[np.all(data != \"NaN\", axis = 1)]\n",
    "    \n",
    "    #Remove punctuation(except '.','?','!') and new lines\n",
    "    for i in range(3):\n",
    "        for j in range(len(data)):\n",
    "            data[j][i+1] = data[j][i+1].replace(\"\\n\",\"\").translate(str.maketrans('', '', remove))\n",
    "    \n",
    "    return feature_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowsToDataPoints(data, embedding_model, model_vocab, num_expected_columns):\n",
    "    #Convert all rows in data to relevant data_points\n",
    "    data_points_with_labels_list = []\n",
    "    for row in data:\n",
    "        convertedRow = convertRowToDataPoint(row, embedding_model, model_vocab)\n",
    "        if len(convertedRow) == num_expected_columns:\n",
    "            data_points_with_labels_list.append(convertRowToDataPoint(row, embedding_model, model_vocab))\n",
    "        \n",
    "    data_points_with_labels = np.array(data_points_with_labels_list)\n",
    "    return data_points_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToDataPoint(row, embedding_model, model_vocab):\n",
    "    #Take in row and convert to data point\n",
    "    #Stores text points then title points then label\n",
    "    curr_title = row[1]\n",
    "    curr_text = row[3]\n",
    "    data_point = np.array([row[4]])\n",
    "    data_point = np.append(getAverageEmbedding(curr_title, embedding_model, model_vocab), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_text, embedding_model, model_vocab), data_point)\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToDataPointWithoutLabel(row, embedding_model, model_vocab):\n",
    "    #Take in row and convert to data point\n",
    "    #Stores text points then title points then label\n",
    "    curr_title = row[1]\n",
    "    curr_text = row[3]\n",
    "    data_point = np.array([])\n",
    "    data_point = np.append(getAverageEmbedding(curr_title, embedding_model, model_vocab), data_point)\n",
    "    data_point = np.append(getAverageEmbedding(curr_text, embedding_model, model_vocab), data_point)\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageEmbedding(curr_string, embedding_model, model_vocab):\n",
    "    #Take in string that has already been cleaned of punctuation and newlines\n",
    "    total_words = 0.0\n",
    "    stop_words = ['not','you','at','from','of','us','in','have','yes','no','are','','for','but','that','it','this','he','she','they','that','a','an','who','where','there','his','her','their','i','my','we','our','were','the','if','as','and','in','on','we','to','also','so','is','its']\n",
    "    remove = string.punctuation\n",
    "    curr_embedding = np.array([])\n",
    "    for k in word_tokenize(curr_string):\n",
    "        curr_word = k + ''\n",
    "        #remove all punctuation from word\n",
    "        curr_word = curr_word.translate(str.maketrans('', '', remove))\n",
    "        #convert to lowercase\n",
    "        curr_word = curr_word.lower()\n",
    "        #check if it is a stop word or empty\n",
    "        if curr_word in stop_words:\n",
    "            continue\n",
    "        #check if word is in model_vocab\n",
    "        if curr_word not in model_vocab:\n",
    "            continue\n",
    "        \n",
    "        if len(curr_embedding) == 0:\n",
    "            curr_embedding = embedding_model[curr_word]\n",
    "        else:\n",
    "            curr_embedding = curr_embedding+embedding_model[curr_word]\n",
    "        \n",
    "        total_words = total_words + 1.0\n",
    "    \n",
    "    curr_embedding = curr_embedding/total_words\n",
    "    return curr_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset (start off with a small set, then work your way up). \n",
    "# Convert all data to numpy array, because they are faster and easier to work with.\n",
    "name = 'twentyPercent.csv'\n",
    "data = loadData(name)\n",
    "data = data.to_numpy()\n",
    "# print(data[:5])\n",
    "\n",
    "# Clean the data so that it can be used in training (This refers to removing rows that contain NaN, removing punctuation, and removing \\n(newlines))\n",
    "# Use cleaned data to train our embedding model. The embedding model is what is used to convert the words to meaningful vectors. \n",
    "# I give the function the data and the number of dimensions I want my vector to be after converting the word.\n",
    "feature_names, data = getCleanedData(data)\n",
    "EmbeddingModel = getEmbeddingModel(data, 100) # test\n",
    "model_vocab = EmbeddingModel.wv.vocab # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demonstration of converting our data to a usable format(i.e from words to vectors)\n",
    "trainData = data\n",
    "trainData2 = data\n",
    "#validationData = data[100:150]\n",
    "\n",
    "# Send data to method in order to convert it. Passed through the data, the embedding model we've previously trained,\n",
    "# the vocabulary of words in our embedded model(to check if we can actually convert a word to a vector or not), \n",
    "# and the number of columns we expect in the matrix that will be returned(at the moment I am calculating this\n",
    "# by using the formula 2*num_dimensions_used_in_embedding + 1).\n",
    "trainData = convertRowsToDataPoints(trainData, EmbeddingModel, model_vocab, 201)\n",
    "trainData2 = convertRowsToDataPoints(trainData2, EmbeddingModel, model_vocab, 201)\n",
    "#validationData = convertRowsToDataPoints(validationData, EmbeddingModel, model_vocab, 201)\n",
    "\n",
    "#Split y_labels from data to store separately\n",
    "trainData_y = trainData[:,200]\n",
    "trainData2_y = trainData2[:,200]\n",
    "\n",
    "#validationData_y = validationData[:,200]\n",
    "\n",
    "#Delete y_labels from data\n",
    "trainData = np.delete(trainData, 200, 1)\n",
    "trainData2 = np.delete(trainData2, 200, 1)\n",
    "#validationData = np.delete(validationData, 200, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation\n",
    "\n",
    "Logistic regression will be accomplished using implementations of gradient descent (with loss minimization) and gradient ascent (maximum likelihood estimation). \n",
    "\n",
    "Reference to the website that helped a lot with the functions:\n",
    "\n",
    "Kaggle.com. 2020. Logistic Regression From Scratch - Python. [online] Available at: <https://www.kaggle.com/jeppbautista/logistic-regression-from-scratch-python> [Accessed 17 April 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initialised weights: \n",
      "[0.002547 0.455505 0.833198 0.32285  0.471227 0.691664 0.20063  0.438592\n",
      " 0.092857 0.498754 0.226118 0.164622 0.167248 0.067306 0.098234 0.414902\n",
      " 0.574866 0.77658  0.536624 0.83724  0.569359 0.53191  0.576543 0.229119\n",
      " 0.073783 0.839152 0.470225 0.389975 0.383859 0.329125 0.785839 0.421591\n",
      " 0.872085 0.340861 0.347282 0.282754 0.833054 0.469442 0.139033 0.022233\n",
      " 0.723996 0.437026 0.341358 0.930521 0.202064 0.614144 0.230418 0.723113\n",
      " 0.368244 0.020358 0.658672 0.937855 0.81753  0.113089 0.045062 0.520779\n",
      " 0.537412 0.276543 0.836366 0.806432 0.553816 0.276176 0.123036 0.877664\n",
      " 0.876525 0.092999 0.038918 0.545872 0.288093 0.149676 0.533287 0.176685\n",
      " 0.389104 0.587254 0.900828 0.290443 0.24175  0.597074 0.231443 0.719329\n",
      " 0.427682 0.09527  0.952606 0.650805 0.001961 0.356822 0.759112 0.736263\n",
      " 0.760185 0.807366 0.099574 0.95916  0.079767 0.402824 0.115044 0.993621\n",
      " 0.233729 0.506369 0.722414 0.080834 0.727357 0.904555 0.030632 0.672156\n",
      " 0.338365 0.147147 0.727519 0.637541 0.360279 0.291214 0.457994 0.86592\n",
      " 0.700097 0.338553 0.479726 0.557455 0.855454 0.435218 0.571968 0.32117\n",
      " 0.290691 0.576963 0.87786  0.456762 0.768718 0.434886 0.675742 0.11471\n",
      " 0.049551 0.91253  0.928417 0.716702 0.781308 0.755551 0.103179 0.107239\n",
      " 0.488907 0.50523  0.064873 0.805184 0.380626 0.244708 0.926005 0.883675\n",
      " 0.052122 0.602968 0.758892 0.981552 0.380755 0.822345 0.463384 0.940242\n",
      " 0.218441 0.154528 0.75469  0.315584 0.829928 0.471892 0.81315  0.327822\n",
      " 0.714287 0.139267 0.310091 0.552722 0.646856 0.120965 0.718268 0.526992\n",
      " 0.783105 0.076453 0.659964 0.777661 0.893584 0.593022 0.117825 0.969245\n",
      " 0.012868 0.892143 0.783381 0.6816   0.700102 0.719456 0.89152  0.479906\n",
      " 0.934794 0.41789  0.418333 0.791409 0.840935 0.566607 0.26575  0.820307\n",
      " 0.147565 0.301025 0.391526 0.46495  0.126921 0.829813 0.557441 0.451917\n",
      " 0.317691 0.076906 0.748024]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(X, weight):\n",
    "    z = np.dot(X, weight)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# The following functions will be used in the implementation of loss minimizing with the gradient descent.\n",
    "def loss(h, y):\n",
    "    loss_value = (-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    return loss_value.mean()\n",
    "\n",
    "def gradient_descent(X, h, y):\n",
    "    descent_var = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "    return descent_var\n",
    "\n",
    "def update_weight_loss(weights, alpha, m):\n",
    "    return weights - alpha * m\n",
    "\n",
    "#The following functions will be used in the implementation of the maximum likelihood estimation with gradient ascent.\n",
    "def log_likelihood(x, y, weights):\n",
    "    z = np.dot(x, weights)\n",
    "    loglikelihood = np.sum( y*z - np.log(1 + np.exp(z)) )\n",
    "    return loglikelihood\n",
    "\n",
    "def gradient_ascent(X, h, y):\n",
    "    ascent_var = np.dot(X.T, y - h)\n",
    "    return ascent_var\n",
    "\n",
    "def update_weight_mle(weights, alpha, m):\n",
    "    return weights + alpha * m\n",
    "\n",
    "# The following code will be used to randomly initialise the weights that will be used in both implementations of gradient descent.\n",
    "import random\n",
    "theta = np.zeros(trainData.shape[1]+1)\n",
    "for i in range(theta.shape[0]):\n",
    "    theta[i] = random.random()\n",
    "print(\"Randomly initialised weights: \")\n",
    "print(np.round(theta,6),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 105.71566343307495 seconds\n",
      "Learning rate: 0.8\n",
      " \n",
      "Weights after the loss minimization implementation of gradient descent:\n",
      "[ 4.5261960e+00  4.9791540e+00  5.3568460e+00 -5.5112800e-01\n",
      " -3.1706620e+00 -6.0999120e+00 -8.0018100e-01 -7.7587190e+00\n",
      "  4.1801210e+00  6.8479410e+00  2.6096300e-01 -3.9622570e+00\n",
      " -7.8212360e+00  9.4442000e-02  2.5623290e+00 -3.6251700e+00\n",
      " -5.5728700e-01  4.4006660e+00  1.6382660e+00 -2.8467030e+00\n",
      " -3.6355860e+00 -5.5092650e+00 -5.0950860e+00  2.6589690e+00\n",
      " -3.7890610e+00  2.8964650e+00 -3.8715990e+00 -5.0458000e-02\n",
      "  3.2020020e+00 -3.5035100e-01 -3.3975510e+00 -1.4896140e+00\n",
      "  5.3471260e+00 -3.8347000e-01  8.3634950e+00 -2.2543640e+00\n",
      "  4.6744380e+00 -4.9503800e-01  2.9822630e+00 -3.4195880e+00\n",
      "  7.3775730e+00  1.8396630e+00 -4.4030430e+00  4.0442050e+00\n",
      "  4.0389200e-01  4.0773140e+00 -4.8120640e+00  4.9255950e+00\n",
      "  1.0143467e+01 -2.9652850e+00 -7.3819800e-01  5.1947360e+00\n",
      " -2.6457340e+00 -8.7007150e+00 -3.3072800e-01 -2.0042450e+00\n",
      " -4.9749200e-01 -3.4166800e-01  6.4061970e+00 -4.9565330e+00\n",
      "  6.7165500e-01 -8.4485800e-01  5.4150900e+00 -1.7249500e-01\n",
      "  3.3787180e+00  6.3246780e+00 -1.1880000e-01  3.5973200e-01\n",
      "  4.3138030e+00 -6.4448800e-01 -3.9560540e+00 -4.7518500e-01\n",
      " -1.6849730e+00 -5.1971700e-01  1.2323740e+00  3.0146860e+00\n",
      "  6.1326570e+00 -1.7141190e+00  1.6375060e+00 -8.1820410e+00\n",
      " -9.9447300e-01 -4.6017850e+00  6.2490880e+00 -4.1435970e+00\n",
      " -1.5615530e+00  6.5772640e+00  3.2066240e+00  6.3582070e+00\n",
      " -7.8634410e+00  5.0047000e-01 -4.5446450e+00 -1.6500600e+00\n",
      "  8.2126700e-01 -4.3774670e+00  6.5851620e+00  1.7639450e+00\n",
      "  9.2960900e-01  3.1587150e+00 -4.6667510e+00 -4.6329190e+00\n",
      " -2.2596900e+00  5.8162430e+00 -2.2648130e+00 -1.3064060e+00\n",
      " -5.4467600e-01  3.8168400e+00 -4.2542460e+00  1.9456040e+00\n",
      " -1.4947130e+00 -5.5787100e-01  2.4015980e+00  4.4311080e+00\n",
      " -1.5739760e+00  1.5356830e+00 -2.5784870e+00  1.2018210e+00\n",
      "  1.1666430e+00 -2.5353580e+00  1.9207970e+00 -1.3799040e+00\n",
      "  2.0266430e+00 -6.7461500e-01 -1.5468260e+00 -4.0099400e-01\n",
      " -1.1619980e+00 -2.3851500e+00 -5.8035000e-02 -3.9805320e+00\n",
      "  1.3638980e+00 -2.9997200e-01  1.8425100e+00 -2.7355580e+00\n",
      "  4.0219000e-02  2.9467730e+00 -3.2904780e+00  2.1576200e+00\n",
      "  6.9296900e-01  1.6657430e+00 -2.3369030e+00 -2.3244670e+00\n",
      "  1.0748320e+00  3.6724500e+00 -3.7311800e-01  9.8246500e-01\n",
      "  6.2410000e-03 -1.2548390e+00  2.8947170e+00 -3.0574320e+00\n",
      " -2.8105640e+00  2.8712010e+00  1.6299970e+00 -2.3683300e+00\n",
      " -1.6875510e+00  2.3613570e+00  8.0588000e-02  3.2626820e+00\n",
      " -8.9963700e-01 -3.1726500e-01 -2.5476300e+00 -9.7859800e-01\n",
      "  3.4343230e+00  1.9475840e+00  4.8332800e-01 -4.5047800e-01\n",
      "  1.4249070e+00 -2.9708810e+00 -1.4348300e+00  2.1846810e+00\n",
      " -6.9638400e-01 -1.0927380e+00 -3.2953650e+00  2.1232050e+00\n",
      "  2.5759700e-01  1.1752210e+00  8.5561200e-01 -4.7163630e+00\n",
      "  3.1938760e+00 -2.4666170e+00  2.0395710e+00 -1.7006190e+00\n",
      "  1.7853140e+00 -3.8583300e-01  1.0403500e+00 -4.1100000e-02\n",
      " -1.1325580e+00 -2.0129720e+00  6.9066700e-01  2.9132130e+00\n",
      "  3.8693100e+00 -5.3388810e+00  2.7502930e+00  5.9477000e-02\n",
      " -6.7217200e-01  4.6057100e-01 -1.3456910e+00  1.7150010e+00\n",
      "  4.0924280e+00 -4.7557100e-01 -2.2783330e+00 -3.8945080e+00\n",
      " -1.3937380e+00  8.5954000e-02 -7.3422400e-01]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of loss minimization with gradient descent:\n",
    "start = time.time()\n",
    "iterations = 100000\n",
    "\n",
    "intercept = np.ones((trainData.shape[0], 1)) \n",
    "trainData = np.concatenate((intercept, trainData), axis=1)\n",
    "theta_lm = theta # Weights that'll be used for the loss minimization implementation\n",
    "\n",
    "for i in range(iterations):\n",
    "    h = sigmoid(trainData, theta_lm)\n",
    "    gradient = gradient_descent(trainData, h, trainData_y)\n",
    "    theta_lm = update_weight_loss(theta_lm, 0.8, gradient)\n",
    "    \n",
    "print(\"Training time: \" + str(time.time() - start) + \" seconds\")\n",
    "print(\"Learning rate: \" + str(0.8))\n",
    "\n",
    "# Illustrations of loss minimization with gradient descent, as well as the updated weights:\n",
    "result = sigmoid(trainData, theta_lm)\n",
    "f = pd.DataFrame(np.around(result, decimals=6)) \n",
    "f['Label'] = trainData_y\n",
    "f['Classifier'] = f[0].apply(lambda x : 0 if x < 0.5 else 1)\n",
    "f.columns = ['Value','Label','Classifier']\n",
    "print(\" \")\n",
    "print(\"Weights after the loss minimization implementation of gradient descent:\")\n",
    "print(np.round(theta_lm,6),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Label</th>\n",
       "      <th>Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.893044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.308560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.999873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.983224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.994597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4017</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4018</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4019</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4021</td>\n",
       "      <td>0.915392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4022 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Value  Label  Classifier\n",
       "0     0.893044    1.0           1\n",
       "1     0.308560    0.0           0\n",
       "2     0.999873    1.0           1\n",
       "3     0.983224    1.0           1\n",
       "4     0.994597    1.0           1\n",
       "...        ...    ...         ...\n",
       "4017  0.010848    0.0           0\n",
       "4018  0.000511    0.0           0\n",
       "4019  0.069096    0.0           0\n",
       "4020  0.000000    0.0           0\n",
       "4021  0.915392    1.0           1\n",
       "\n",
       "[4022 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function of the implementation of loss minimization with gradient descent:\n",
    "result.sort()\n",
    "x = np.zeros(trainData.shape[0])\n",
    "for i in range(trainData.shape[0]):\n",
    "    x[i] = i\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(z)\")\n",
    "plt.plot(x,result)\n",
    "plt.axhline(y=0.5, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[1944  104]\n",
      " [  77 1897]]\n",
      "Accuracy Score : 0.9549975136747887\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.95      0.96      2048\n",
      "         1.0       0.95      0.96      0.95      1974\n",
      "\n",
      "    accuracy                           0.95      4022\n",
      "   macro avg       0.95      0.96      0.95      4022\n",
      "weighted avg       0.96      0.95      0.96      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the implementation of loss minimization with gradient descent:\n",
    "# (Thanks to the help of the scikit-learn library).\n",
    "actual = f['Label']\n",
    "predicted = f['Classifier']\n",
    "cf = confusion_matrix(actual, predicted)\n",
    "\n",
    "print('Confusion Matrix :')\n",
    "print(cf) \n",
    "print('Accuracy Score :',accuracy_score(actual, predicted)) \n",
    "print('Report : ')\n",
    "print(classification_report(actual, predicted)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 151.2077512741089 seconds\n",
      "Learning rate: 0.5\n",
      " \n",
      "Weights after the maximum likelihood estimation implementation of gradient ascent:\n",
      "[  7046.150991   7046.150991  -1113.332136  -4830.573828  -9784.339699\n",
      "   -635.85943   -9935.953705   4516.882078   9753.054081  -1827.350492\n",
      "  -4539.346366  -6470.805456    193.020822    -53.720078  -5167.970339\n",
      "   -831.951557   6039.365325   2033.895995  -1267.342432  -3638.519713\n",
      "  -4614.872604  -4574.821517   3157.746353  -5514.51805    5229.586769\n",
      "  -3227.172759    769.452161   2451.155363  -1497.994695  -7244.020748\n",
      "  -1118.261057   7293.618833  -1150.614523  11052.996149  -5555.716576\n",
      "   5040.262261  -2623.544111   2603.435818  -3203.848463   9775.516211\n",
      "   3799.931292  -7299.368823   2685.098881   2542.412322   5456.74481\n",
      "  -6466.252925   4062.47901   10614.137912  -2938.434801  -1086.943165\n",
      "   5620.753728  -6496.975232 -12891.045659  -3043.111454  -3214.166184\n",
      "   1401.367139    356.421621   4110.189761  -5527.488948   -452.077505\n",
      "   -112.144218   4323.798945   -330.946942   2031.770083   6795.700614\n",
      "    250.325271    251.196422   5309.300827    261.57113   -6527.08955\n",
      "  -1053.728893   -435.69255   -2495.854676   2271.129947   3028.796455\n",
      "   6946.006967    492.059364   1059.783295 -10132.345464  -1724.227798\n",
      "  -3385.681162   6511.484472  -2525.067962  -1607.35064    5999.069536\n",
      "   3860.567274   4889.68577   -8886.238011   1347.391359  -3416.083358\n",
      "  -1227.402075   1094.949743  -4265.922778   8696.319688   1795.502854\n",
      "   1215.767133   3512.987313  -5339.733933  -6579.412937  -4615.597059\n",
      "   7789.944911  -2124.383468  -2343.997946    567.186976   4391.53113\n",
      "  -5046.924318   3108.209375  -1756.174563     33.461287   3113.592882\n",
      "   4221.830784  -1479.882671   1369.123871  -2199.746867   1372.495939\n",
      "   1257.676441  -2723.389084   2162.221358  -1971.874752   2033.318172\n",
      "  -1503.396441  -1767.954034   -376.661994   -602.656032  -2231.485876\n",
      "    756.0484    -4902.163096    614.090931     47.091339   2444.283468\n",
      "  -3006.227135     43.493664   2858.100147  -3248.928731   2830.998279\n",
      "   1180.032378   1547.1278    -3158.469375  -4245.16916     966.691383\n",
      "   4685.069591   -618.141334   1480.683733    523.642748  -2019.748556\n",
      "   3036.716685  -3032.400066  -3493.117467   2337.334207   1473.495681\n",
      "  -1845.503503   -204.994736   2813.383732   -419.703382   2927.090131\n",
      "   -984.448306   -248.195928  -2652.493108  -1191.942869   4525.402852\n",
      "   2510.643415   1479.814294  -1047.902389   1946.380576  -3320.004719\n",
      "  -2599.486626   2320.554798  -1080.273903   -467.779939  -3137.158919\n",
      "   1731.208845    855.831945   1682.719359   1279.295292  -5461.533029\n",
      "   3816.580405  -3374.514675   2204.943421  -2024.861096   1403.385689\n",
      "   -423.974029   1450.091419   -371.89951   -1158.383549  -2821.871461\n",
      "    965.962992   3473.484484   4629.189045  -6363.952966   3334.615468\n",
      "     72.019272  -1273.135896    -59.800752  -1845.3902     2290.646614\n",
      "   3343.709827  -1488.473453  -2074.445553  -3455.843791  -1511.911883\n",
      "    168.442471  -2166.438484]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of the maximum likelihood estimation with gradient ascent: \n",
    "start = time.time()\n",
    "iterations = 100000\n",
    "\n",
    "intercept2 = np.ones((trainData2.shape[0], 1))\n",
    "trainData2 = np.concatenate((intercept2, trainData2), axis=1)\n",
    "theta_ml = np.zeros(trainData2.shape[1]) # Weights that'll be used for the loss maximimum likelihood estimation\n",
    "\n",
    "for i in range(iterations):\n",
    "    h2 = sigmoid(trainData2, theta_ml)\n",
    "    gradient2 = gradient_ascent(trainData2, h2, trainData2_y)\n",
    "    theta_ml = update_weight_mle(theta_ml, 0.5, gradient2)\n",
    "    \n",
    "print(\"Training time: \" + str(time.time() - start) + \" seconds\")\n",
    "print(\"Learning rate: \" + str(0.5))\n",
    "\n",
    "# Illustrations of the maximum likelihood estimation with gradient ascent, as well as the updated weights:\n",
    "result2 = sigmoid(trainData2, theta_ml)\n",
    "f2 = pd.DataFrame(np.around(result2, decimals=6))\n",
    "f2['Label'] = trainData2_y\n",
    "f2['Classifier'] = f2[0].apply(lambda x : 0 if x < 0.5 else 1)\n",
    "f2.columns = ['Value','Label','Classifier']\n",
    "print(\" \")\n",
    "print(\"Weights after the maximum likelihood estimation implementation of gradient ascent:\")\n",
    "print(np.round(theta_ml,6),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Label</th>\n",
       "      <th>Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4022 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value  Label  Classifier\n",
       "0       1.0    1.0           1\n",
       "1       0.0    0.0           0\n",
       "2       1.0    1.0           1\n",
       "3       1.0    1.0           1\n",
       "4       1.0    1.0           1\n",
       "...     ...    ...         ...\n",
       "4017    0.0    0.0           0\n",
       "4018    0.0    0.0           0\n",
       "4019    0.0    0.0           0\n",
       "4020    0.0    0.0           0\n",
       "4021    1.0    1.0           1\n",
       "\n",
       "[4022 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sigmoid function of the implementation of the maximum likelihood estimation with gradient descent:\n",
    "result2.sort()\n",
    "x = np.zeros(trainData.shape[0])\n",
    "for i in range(trainData.shape[0]):\n",
    "    x[i] = i\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(z)\")\n",
    "plt.plot(x,result2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[2015   33]\n",
      " [ 272 1702]]\n",
      "Accuracy Score : 0.9241670810542019\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93      2048\n",
      "         1.0       0.98      0.86      0.92      1974\n",
      "\n",
      "    accuracy                           0.92      4022\n",
      "   macro avg       0.93      0.92      0.92      4022\n",
      "weighted avg       0.93      0.92      0.92      4022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy of the implementation of the maximum likelihood estimation with gradient ascent:\n",
    "# (Thanks to the help of the scikit-learn library).\n",
    "\n",
    "actual2 = f2['Label']\n",
    "predicted2 = f2['Classifier']\n",
    "cf2 = confusion_matrix(actual2, predicted2)\n",
    "\n",
    "print('Confusion Matrix :')\n",
    "print(cf2) \n",
    "print('Accuracy Score :',accuracy_score(actual2, predicted2)) \n",
    "print('Report : ')\n",
    "print(classification_report(actual2, predicted2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
